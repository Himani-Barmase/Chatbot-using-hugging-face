# -*- coding: utf-8 -*-
"""Local_Command-Line_Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16NbEFGBLsuRtsxfwc_xxh3U3TubLjZUH

# Local Command-Line Chatbot using Hugging Face
"""

# Create project folder
import os

project_path = "/content/chatbot"
os.makedirs(project_path, exist_ok=True)

# Create required files
files = ["model_loader.py", "chat_memory.py", "interface.py", "README.md"]
for f in files:
    open(os.path.join(project_path, f), "w").close()

print("Project structure created at:", project_path)

import os

# Create folder and files
base = "/content/chatbot"
os.makedirs(base, exist_ok=True)
filenames = ["model_loader.py", "chat_memory.py", "interface.py", "README.md"]
for fn in filenames:
    open(os.path.join(base, fn), "w").close()

# Add code to each file
code_dict = {
    "model_loader.py": '''
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

def load_chatbot_model(model_name: str = "distilgpt2"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=100,
        pad_token_id=tokenizer.eos_token_id
    )
    return generator
''',

    "chat_memory.py": '''
from collections import deque

class ChatMemory:
    def __init__(self, window_size: int = 5):
        self.window_size = window_size
        self.memory = deque(maxlen=window_size)

    def add_turn(self, speaker: str, text: str):
        self.memory.append(f"{speaker}: {text}")

    def get_context(self) -> str:
        return "\\n".join(self.memory)
''',

    "interface.py": '''
from model_loader import load_chatbot_model
from chat_memory import ChatMemory

def run_chat():
    generator = load_chatbot_model("distilgpt2")
    memory = ChatMemory(window_size=5)

    print("Local CLI Chatbot (type /exit to quit)\\n")

    while True:
        user_input = input("User: ")

        if user_input.strip().lower() == "/exit":
            print("Exiting chatbot. Goodbye!")
            break

        memory.add_turn("User", user_input)
        prompt = memory.get_context() + "\\nBot:"
        response = generator(prompt, max_new_tokens=50, num_return_sequences=1)[0]["generated_text"]
        bot_reply = response.split("Bot:")[-1].strip().split("\\n")[0]

        print(f"Bot: {bot_reply}")
        memory.add_turn("Bot", bot_reply)

if __name__ == "__main__":
    run_chat()
''',

    "README.md": '''
# Local Command-Line Chatbot (LLM - Hugging Face)
##  Run
python interface.py
'''
}

for fname, code in code_dict.items():
    with open(os.path.join(base, fname), "w") as f:
        f.write(code)

print("âœ… All files created and code added.")

!python /content/chatbot/interface.py

# Make a zip file of the chatbot folder
!zip -r chatbot.zip /content/chatbot

from google.colab import files
files.download('chatbot.zip')



